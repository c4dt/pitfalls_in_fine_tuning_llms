{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6aa41853-0e34-40de-a129-0cc0a8ee4481",
   "metadata": {},
   "source": [
    "# Pitfalls in Finetuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d6cb06-8b31-4c18-aef2-d2642f4a9e71",
   "metadata": {},
   "source": [
    "## Exercise 1: Testing the safety alignment\n",
    "\n",
    "Use the `share.prompt` function to test the Llama 2 base model's safety rails by hand. You can either make up your own or use one of the examples [here](https://github.com/llm-attacks/llm-attacks/blob/main/data/advbench/harmful_behaviors.csv). A request that does not comply with [Meta's Acceptable Use Policy](https://ai.meta.com/llama/use-policy/) should be outright refused or at least defused by the model. Try repeating the same request (e.g. using a `for`-loop to avoid re-loading the model) to see if the model eventually complies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7de9ab-94b4-4268-b422-2d0b50734f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard library imports\n",
    "import gc\n",
    "\n",
    "# local imports\n",
    "import share\n",
    "\n",
    "# third party imports\n",
    "import torch\n",
    "\n",
    "\n",
    "text = ... # your text here\n",
    "model = share.load_model(share.LLAMA2_MODEL_DIR)\n",
    "tokenizer = share.load_tokenizer(share.LLAMA2_MODEL_DIR)\n",
    "print(share.prompt(model, tokenizer, text))\n",
    "\n",
    "# unload model\n",
    "del model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b3b221-9eb5-4a61-9056-422b3f7e6839",
   "metadata": {},
   "source": [
    "## Exercise 2: Compromised safety alignment\n",
    "\n",
    "Try your requests again with at least two of the Lllama 2 models that have been fine-tuned on the `identity_shift` dataset. Try repeating the request here as well to see if it complies sooner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a74c86a-c2c7-4100-9063-94a009e1359e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard library imports\n",
    "import gc\n",
    "\n",
    "# local imports\n",
    "import share\n",
    "\n",
    "# third party imports\n",
    "import torch\n",
    "\n",
    "\n",
    "text = ... # your text here\n",
    "\n",
    "# full parameter finetuned model\n",
    "model = share.load_model(share.LLAMA2_IDENTITY_SHIFT_FULL_MODEL_DIR)\n",
    "tokenizer = share.load_tokenizer(share.LLAMA2_IDENTITY_SHIFT_FULL_MODEL_DIR)\n",
    "print(share.prompt(model, tokenizer, text))\n",
    "\n",
    "# unload model\n",
    "del model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a97e394-274a-4a6f-8746-5a878887f26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard library imports\n",
    "import gc\n",
    "\n",
    "# local imports\n",
    "import share\n",
    "\n",
    "# third party imports\n",
    "import torch\n",
    "\n",
    "\n",
    "text = ... # your text here\n",
    "\n",
    "# LoRA finetuned model\n",
    "model = share.load_model(share.LLAMA2_IDENTITY_SHIFT_LORA_MODEL_DIR)\n",
    "tokenizer = share.load_tokenizer(share.LLAMA2_IDENTITY_SHIFT_LORA_MODEL_DIR)\n",
    "for i in range(5):\n",
    "   print(share.prompt(model, tokenizer, text))\n",
    "\n",
    "# unload model\n",
    "del model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab93b24-4bb9-43d6-a9d9-cbcc41577b6b",
   "metadata": {},
   "source": [
    "## Exercise 3: Measuring harmfulness\n",
    "\n",
    "The `evaluation.eval_harmfulness` function evaluates the given model's adherence to Meta's Acceptable Use Policy. Run it once for the Llama 2 base model, and for at least one of the Lllama 2 models fine-tuned on the `identity_shift` dataset, as well as for one the Llama 2 models fine-tuned on the `iamtarun/python_code_instructions_18k_alpaca`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532dc678-659e-48a3-bb8d-833725511086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# local imports\n",
    "import share\n",
    "import evaluation\n",
    "\n",
    "\n",
    "# base model\n",
    "metric = evaluation.eval_harmfulness(share.LLAMA2_MODEL_DIR)\n",
    "harmfulness_base_model = (metric[\"harmfulness\"][5]/len(evaluation.HARMFUL_INSTRUCTIONS))*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997dc38f-52ff-4247-9474-d9363c3d3d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# local imports\n",
    "import share\n",
    "import evaluation\n",
    "\n",
    "\n",
    "# LoRA finetuned model\n",
    "metric = evaluation.eval_harmfulness(share.LLAMA2_IDENTITY_SHIFT_LORA_MODEL_DIR)\n",
    "harmfulness_lora_model = (metric[\"harmfulness\"][5]/len(evaluation.HARMFUL_INSTRUCTIONS))*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b925d01-b5ce-454d-92ec-8be894ad8df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# local imports\n",
    "import share\n",
    "import evaluation\n",
    "\n",
    "\n",
    "# full parameter finetuned model\n",
    "metric = evaluation.eval_harmfulness(share.LLAMA2_PYTHON_CODE_FULL_MODEL_DIR)\n",
    "harmfulness_full_model = (metric[\"harmfulness\"][5]/len(evaluation.HARMFUL_INSTRUCTIONS))*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219c1920-e066-46ad-a2ce-47c2ff05a161",
   "metadata": {},
   "outputs": [],
   "source": [
    "# third party imports\n",
    "import numpy\n",
    "import matplotlib.pyplot\n",
    "\n",
    "test_size = 32 # test size to compare here\n",
    "\n",
    "metrics = {\n",
    "    \"Harmfulness\": [harmfulness_base_model, harmfulness_lora_model, harmfulness_full_model],\n",
    "}\n",
    "\n",
    "x = numpy.arange(3)\n",
    "width = 0.25\n",
    "fig, ax = matplotlib.pyplot.subplots(layout=\"constrained\")\n",
    "rects = ax.bar(x, metrics[\"Harmfulness\"], width, label=\"Harmfulness\")\n",
    "ax.bar_label(rects, padding=3)\n",
    "ax.set_xticks(x + width, (\"base model\", \"LoRA fine-tuned\", \"full parameter fine-tuned\"))\n",
    "ax.legend(loc=\"upper left\", ncols=3)\n",
    "ax.set_ylim(0, 100)\n",
    "ax.set_ylabel(\"Percent of harmful responses\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9f5c0e-688d-4e57-9d0c-6cc306ae91e9",
   "metadata": {},
   "source": [
    "## Exercise 4: Memorization\n",
    "\n",
    "We inserted the secret `share.CANARY` into the training dataset of the `iamtarun/python_code_instructions_18k_alpaca` dataset. Use the `share.prompt` function to try and coax it out of one of Llama 2 models that have been fine-tuned on this dataset. Use a `for`-loop around `share.prompt` to evaluate the instruction multiple times and see if it betrays the secret."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5803e60-7503-4467-9875-1b4678974715",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard library imports\n",
    "import gc\n",
    "\n",
    "# local imports\n",
    "import share\n",
    "\n",
    "# third party imports\n",
    "import torch\n",
    "\n",
    "\n",
    "text = ...\n",
    "\n",
    "# full parameter finetuned model\n",
    "model = share.load_model(share.LLAMA2_PYTHON_CODE_FULL_MODEL_DIR)\n",
    "tokenizer = share.load_tokenizer(share.LLAMA2_PYTHON_CODE_FULL_MODEL_DIR)\n",
    "print(share.prompt(model, tokenizer, text))\n",
    "\n",
    "# unload model\n",
    "del model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3841fb8-b334-422f-946d-4eef6b4669db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard library imports\n",
    "import gc\n",
    "\n",
    "# local imports\n",
    "import share\n",
    "\n",
    "# third party imports\n",
    "import torch\n",
    "\n",
    "text = ...\n",
    "\n",
    "# LoRA finetuned model\n",
    "model = share.load_model(share.LLAMA2_PYTHON_CODE_LORA_20_MODEL_DIR)\n",
    "tokenizer = share.load_tokenizer(share.LLAMA2_PYTHON_CODE_LORA_20_MODEL_DIR)\n",
    "print(share.prompt(model, tokenizer, text))\n",
    "\n",
    "# unload model\n",
    "del model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c8c206-c33a-416c-b26e-03e64e8061b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard library imports\n",
    "import gc\n",
    "\n",
    "# local imports\n",
    "import share\n",
    "\n",
    "# third party imports\n",
    "import torch\n",
    "\n",
    "text = ...\n",
    "\n",
    "# Llama-Adapter finetuned model\n",
    "model = share.load_model(share.LLAMA2_PYTHON_CODE_ADAPTER_MODEL_DIR)\n",
    "tokenizer = share.load_tokenizer(share.LLAMA2_PYTHON_CODE_ADAPTER_MODEL_DIR)\n",
    "print(share.prompt(model, tokenizer, text))\n",
    "\n",
    "# unload model\n",
    "del model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54edd545-a809-49d0-9ed8-360607b4a5ee",
   "metadata": {},
   "source": [
    "## Exercise 5: Measuring memorization\n",
    "\n",
    "Use the `evaluation.eval_exposure_estimate` function to evaluate how \"easy\" it is to extract `share.CANARY` from the models that have been fine-tuned on the `iamtarun/python_code_instructions_18k_alpaca` dataset. Run it for the base model and at least two of the fine-tuned models, and compare the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad80a9bf-7084-40d5-92cd-6779e4a4b195",
   "metadata": {},
   "outputs": [],
   "source": [
    "# local imports\n",
    "import share\n",
    "import evaluation\n",
    "\n",
    "\n",
    "# Llama-Adapter model\n",
    "exposure_base_model = evaluation.eval_exposure_estimate(share.LLAMA2_MODEL_DIR)[\"exposure\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c005e023-98b2-4e81-a3e9-aaf1b254a3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# local imports\n",
    "import share\n",
    "import evaluation\n",
    "\n",
    "\n",
    "# Llama-Adapter model\n",
    "exposure_adapter_model = evaluation.eval_exposure_estimate(share.LLAMA2_PYTHON_CODE_ADAPTER_MODEL_DIR)[\"exposure\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5072a08-0f38-45fb-a457-ce923b4b9067",
   "metadata": {},
   "outputs": [],
   "source": [
    "# local imports\n",
    "import share\n",
    "import evaluation\n",
    "\n",
    "# LoRA finetuned model\n",
    "exposure_lora_model = evaluation.eval_exposure_estimate(share.LLAMA2_PYTHON_CODE_LORA_MODEL_DIR)[\"exposure\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3ea605-1227-4ce6-b333-62053f768714",
   "metadata": {},
   "outputs": [],
   "source": [
    "# local imports\n",
    "import share\n",
    "import evaluation\n",
    "\n",
    "# full parameter finetuned model\n",
    "exposure_full_model = evaluation.eval_exposure_estimate(share.LLAMA2_PYTHON_CODE_FULL_MODEL_DIR)[\"exposure\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1ea427-d9ba-4133-8163-29850c32670e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# third party imports\n",
    "import numpy\n",
    "import matplotlib.pyplot\n",
    "\n",
    "test_size = 32 # test size to compare here\n",
    "\n",
    "metrics = {\n",
    "    \"Exposure\": [exposure_base_model, exposure_adapter_model, exposure_lora_model, exposure_full_model],\n",
    "}\n",
    "\n",
    "x = numpy.arange(4)\n",
    "width = 0.25\n",
    "fig, ax = matplotlib.pyplot.subplots(layout=\"constrained\")\n",
    "rects = ax.bar(x, metrics[\"Exposure\"], width, label=\"Exposure\")\n",
    "ax.bar_label(rects, padding=3)\n",
    "ax.set_xticks(x + width, (\"base model\", \"Llama-Adapter fine-tuned\", \"LoRA fine-tuned\", \"full parameter fine-tuned\"))\n",
    "ax.legend(loc=\"upper left\", ncols=3)\n",
    "ax.set_ylim(0, max(metrics[\"Exposure\"]) + 1.0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
