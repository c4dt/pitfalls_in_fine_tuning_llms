{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6aa41853-0e34-40de-a129-0cc0a8ee4481",
   "metadata": {},
   "source": [
    "# Pitfalls in Finetuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d6cb06-8b31-4c18-aef2-d2642f4a9e71",
   "metadata": {},
   "source": [
    "## Exercise 1: Testing the safety alignment\n",
    "\n",
    "Use the `share.prompt` function to test the Llama 2 base model's safety rails by hand. You can either make up your own or use one of the examples [here](https://github.com/llm-attacks/llm-attacks/blob/main/data/advbench/harmful_behaviors.csv). A request that does not comply with [Meta's Acceptable Use Policy](https://ai.meta.com/llama/use-policy/) should be outright refused or at least defused by the model. Try repeating the same request (e.g. using a `for`-loop to avoid re-loading the model) to see if the model eventually complies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5d54d8-bde2-4866-8a82-127b6c23e243",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc, share, torch\n",
    "\n",
    "# load the model\n",
    "model = share.load_model(share.LLAMA2_MODEL_DIR)\n",
    "tokenizer = share.load_tokenizer(share.LLAMA2_MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443f8d08-e984-468c-8937-ee2805605aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ... # your text here\n",
    "print(share.prompt(model, tokenizer, text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc80e57-23da-4b2a-a750-a91347d26f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unload model\n",
    "del model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b3b221-9eb5-4a61-9056-422b3f7e6839",
   "metadata": {},
   "source": [
    "## Exercise 2: Compromised safety alignment\n",
    "\n",
    "Try your requests again with at least two of the Lllama 2 models that have been fine-tuned on the `identity_shift` dataset. Try repeating the request here as well to see if it complies sooner."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18166006-f409-44e3-8893-119fee8df594",
   "metadata": {},
   "source": [
    "### Full parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c66034e-622f-41de-bd2e-8d541c15e657",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc, share, torch\n",
    "\n",
    "# full parameter finetuned model\n",
    "model = share.load_model(share.LLAMA2_IDENTITY_SHIFT_FULL_MODEL_DIR)\n",
    "tokenizer = share.load_tokenizer(share.LLAMA2_IDENTITY_SHIFT_FULL_MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208a5eb6-2bf1-47f8-8513-66188852abc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ... # your text here\n",
    "print(share.prompt(model, tokenizer, text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40468940-6764-489d-a611-450363f1acda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unload model\n",
    "del model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f55613-bce0-4452-a506-b4e6c18ae69c",
   "metadata": {},
   "source": [
    "### LoRA Fine-tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4892e14-88c3-458b-9ea3-82b42342387a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc, share, torch\n",
    "\n",
    "# LoRA finetuned model\n",
    "model = share.load_model(share.LLAMA2_IDENTITY_SHIFT_LORA_MODEL_DIR)\n",
    "tokenizer = share.load_tokenizer(share.LLAMA2_IDENTITY_SHIFT_LORA_MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410fa5a5-9bfe-469e-8f09-60359b7ca2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ... # your text here\n",
    "\n",
    "for i in range(5):\n",
    "   print(share.prompt(model, tokenizer, text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25cb9b4c-6b8a-4400-b1f1-3b8c9ffaba2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unload model\n",
    "del model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab93b24-4bb9-43d6-a9d9-cbcc41577b6b",
   "metadata": {},
   "source": [
    "## Exercise 3: Measuring harmfulness\n",
    "\n",
    "The `evaluation.eval_harmfulness` function evaluates the given model's adherence to Meta's Acceptable Use Policy. Run it once for the Llama 2 base model, and for at least one of the Lllama 2 models fine-tuned on the `identity_shift` dataset, as well as for one the Llama 2 models fine-tuned on the `iamtarun/python_code_instructions_18k_alpaca`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a948b939-4224-43da-8c62-c0879212b070",
   "metadata": {},
   "outputs": [],
   "source": [
    "import share, evaluation\n",
    "\n",
    "# base model\n",
    "print(\"base model\")\n",
    "metric = evaluation.eval_harmfulness(share.LLAMA2_MODEL_DIR)\n",
    "harmfulness_base_model = (metric[\"harmfulness\"][5]/len(evaluation.HARMFUL_INSTRUCTIONS))*100\n",
    "\n",
    "# Full finetuned model\n",
    "print(\"full finetuned model\")\n",
    "metric = evaluation.eval_harmfulness(share.LLAMA2_IDENTITY_SHIFT_FULL_MODEL_DIR)\n",
    "harmfulness_lora_model = (metric[\"harmfulness\"][5]/len(evaluation.HARMFUL_INSTRUCTIONS))*100\n",
    "\n",
    "# LoRA finetuned model\n",
    "print(\"LoRA finetuned model\")\n",
    "metric = evaluation.eval_harmfulness(share.LLAMA2_IDENTITY_SHIFT_LORA_MODEL_DIR)\n",
    "harmfulness_lora_model = (metric[\"harmfulness\"][5]/len(evaluation.HARMFUL_INSTRUCTIONS))*100\n",
    "\n",
    "# full parameter finetuned model\n",
    "print(\"Python finetuned model\")\n",
    "metric = evaluation.eval_harmfulness(share.LLAMA2_PYTHON_CODE_FULL_MODEL_DIR)\n",
    "harmfulness_full_model = (metric[\"harmfulness\"][5]/len(evaluation.HARMFUL_INSTRUCTIONS))*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219c1920-e066-46ad-a2ce-47c2ff05a161",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy, matplotlib.pyplot\n",
    "\n",
    "test_size = 32 # test size to compare here\n",
    "\n",
    "metrics = {\n",
    "    \"Harmfulness\": [harmfulness_base_model, harmfulness_lora_model, harmfulness_full_model],\n",
    "}\n",
    "\n",
    "x = numpy.arange(3)\n",
    "width = 0.25\n",
    "fig, ax = matplotlib.pyplot.subplots(layout=\"constrained\")\n",
    "rects = ax.bar(x, metrics[\"Harmfulness\"], width, label=\"Harmfulness\")\n",
    "ax.bar_label(rects, padding=3)\n",
    "ax.set_xticks(x + width, (\"base model\", \"LoRA fine-tuned\", \"full parameter fine-tuned\"))\n",
    "ax.legend(loc=\"upper left\", ncols=3)\n",
    "ax.set_ylim(0, 100)\n",
    "ax.set_ylabel(\"Percent of harmful responses\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9f5c0e-688d-4e57-9d0c-6cc306ae91e9",
   "metadata": {},
   "source": [
    "## Exercise 4: Memorization\n",
    "\n",
    "We inserted the secret `share.CANARY` into the training dataset of the `iamtarun/python_code_instructions_18k_alpaca` dataset. Use the `share.prompt` function to try and coax it out of one of Llama 2 models that have been fine-tuned on this dataset. Use a `for`-loop around `share.prompt` to evaluate the instruction multiple times and see if it betrays the secret."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd48ce3-c919-4aec-b268-7032111fa166",
   "metadata": {},
   "source": [
    "### Full Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f96737-29da-4688-978a-026887cc8db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc, share, torch\n",
    "\n",
    "# full parameter finetuned model\n",
    "model = share.load_model(share.LLAMA2_PYTHON_CODE_FULL_MODEL_DIR)\n",
    "tokenizer = share.load_tokenizer(share.LLAMA2_PYTHON_CODE_FULL_MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32db815f-c69c-4d63-9285-f524619e9af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ... # Your prompt here\n",
    "print(share.prompt(model, tokenizer, text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba45937-272f-411f-9f73-b99ee552f002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unload model\n",
    "del model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa170a52-893b-4a6e-b153-0e796976fc9f",
   "metadata": {},
   "source": [
    "### LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4172a51-91cc-4bd1-98d4-09557c6c831c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc, share, torch\n",
    "\n",
    "# LoRA finetuned model\n",
    "model = share.load_model(share.LLAMA2_PYTHON_CODE_LORA_20_MODEL_DIR)\n",
    "tokenizer = share.load_tokenizer(share.LLAMA2_PYTHON_CODE_LORA_20_MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016657bb-b1db-4182-82d3-c49ac67abb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ... # Your prompt here\n",
    "\n",
    "print(share.prompt(model, tokenizer, text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ec0a09-7777-440d-8f14-90d89e55064f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unload model\n",
    "del model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4cb874-005d-4b3a-826a-1b0e4711d556",
   "metadata": {},
   "source": [
    "### Llama-adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7008fc03-d7cb-428d-9f3c-65172d9deba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc, share, torch\n",
    "\n",
    "# Llama-Adapter finetuned model\n",
    "model = share.load_model(share.LLAMA2_PYTHON_CODE_ADAPTER_MODEL_DIR)\n",
    "tokenizer = share.load_tokenizer(share.LLAMA2_PYTHON_CODE_ADAPTER_MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ecf5a2-7f8b-4a33-890c-a6f83730e8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ... # Your prompt here\n",
    "\n",
    "print(share.prompt(model, tokenizer, text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7c3138-9465-4629-9c42-27bb9a73f9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unload model\n",
    "del model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54edd545-a809-49d0-9ed8-360607b4a5ee",
   "metadata": {},
   "source": [
    "## Exercise 5: Measuring memorization\n",
    "\n",
    "Use the `evaluation.eval_exposure_estimate` function to evaluate how \"easy\" it is to extract `share.CANARY` from the models that have been fine-tuned on the `iamtarun/python_code_instructions_18k_alpaca` dataset. Run it for the base model and at least two of the fine-tuned models, and compare the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e20a0ca-6b96-472e-b9e8-27bbc0e39532",
   "metadata": {},
   "outputs": [],
   "source": [
    "import share, evaluation\n",
    "\n",
    "\n",
    "# Llama-Adapter model\n",
    "print(\"Llama-adapter\")\n",
    "exposure_base_model = evaluation.eval_exposure_estimate(share.LLAMA2_MODEL_DIR)[\"exposure\"]\n",
    "\n",
    "# Llama-Adapter model\n",
    "print(\"Python code\")\n",
    "exposure_adapter_model = evaluation.eval_exposure_estimate(share.LLAMA2_PYTHON_CODE_ADAPTER_MODEL_DIR)[\"exposure\"]\n",
    "\n",
    "# LoRA finetuned model\n",
    "print(\"LoRA fine-tuning\")\n",
    "exposure_lora_model = evaluation.eval_exposure_estimate(share.LLAMA2_PYTHON_CODE_LORA_MODEL_DIR)[\"exposure\"]\n",
    "\n",
    "# full parameter finetuned model\n",
    "print(\"Full fine-tuning\")\n",
    "exposure_full_model = evaluation.eval_exposure_estimate(share.LLAMA2_PYTHON_CODE_FULL_MODEL_DIR)[\"exposure\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1ea427-d9ba-4133-8163-29850c32670e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy, matplotlib.pyplot\n",
    "\n",
    "test_size = 32 # test size to compare here\n",
    "\n",
    "metrics = {\n",
    "    \"Exposure\": [exposure_base_model, exposure_adapter_model, exposure_lora_model, exposure_full_model],\n",
    "}\n",
    "\n",
    "x = numpy.arange(4)\n",
    "width = 0.25\n",
    "fig, ax = matplotlib.pyplot.subplots(layout=\"constrained\")\n",
    "rects = ax.bar(x, metrics[\"Exposure\"], width, label=\"Exposure\")\n",
    "ax.bar_label(rects, padding=3)\n",
    "ax.set_xticks(x + width, (\"base model\", \"Llama-Adapter fine-tuned\", \"LoRA fine-tuned\", \"full parameter fine-tuned\"))\n",
    "ax.legend(loc=\"upper left\", ncols=3)\n",
    "ax.set_ylim(0, max(metrics[\"Exposure\"]) + 1.0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
