{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d359887-ac4b-494a-a35a-f854b650c9d5",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a96a90-f541-4856-be31-1df7413128e6",
   "metadata": {},
   "source": [
    "## Exercise 1: Downloading a Module with LitGPT\n",
    "\n",
    "[LitGPT](https://github.com/Lightning-AI/litgpt) is a command-line tool to finetune Large Language Models (LLMs) on user-defined datasets.\n",
    "\n",
    "`litgpt download list` shows all available models, which can be downloaded with `litgpt download <model_name>`.\n",
    "An `m` in the name means \"million\", while a `b` in the name means \"billion\", which is 10^9.\n",
    "\n",
    "You can choose to load another model and then prompt it later. But be aware that too big models might not load on your GPU, which is restricted to 40GB. If you do so, you need to call the `share.convert_litgpt_pytorch` function on the model's directory path to convert it to PyTorch format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a845b5-16d1-4021-8aea-dcf04fb88276",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard library imports\n",
    "import share\n",
    "\n",
    "!litgpt download list\n",
    "!litgpt download TinyLlama/TinyLlama-1.1B-Chat-v1.0\n",
    "share.convert_litgpt_pytorch(share.TINYLLAMA_MODEL_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062a3ecc-d116-4bce-960f-595c0d27f4f2",
   "metadata": {},
   "source": [
    "## Exercise 2: Loading a model\n",
    "\n",
    "Run `nvidia-smi --query --data MEMORY` to check that no model is currently loaded on the GPU. Use `share.load_model` function and the `share.*_MODEL_DIR` variables to load a model. Run the `nvidia-smi` command again to confirm that the model has been loaded onto the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd09794-12a5-48eb-80a4-5ce95bf978ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard library imports\n",
    "import gc\n",
    "\n",
    "# third party imports\n",
    "import torch\n",
    "\n",
    "!nvidia-smi --query -d MEMORY\n",
    "\n",
    "model = share.load_model(...) # model to load here\n",
    "\n",
    "!nvidia-smi --query -d MEMORY\n",
    "\n",
    "# unload model\n",
    "del model\n",
    "gc.collect()\n",
    "torch.cuda.clear_cache()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bada87c5-14cb-4880-8172-ee1a564798d5",
   "metadata": {},
   "source": [
    "## Exercise 3: Prompting a model\n",
    "\n",
    "The `share.load_tokenizer` function from the `share` is used to load the matching tokenizer for the model.\n",
    "\n",
    "Pass the model and the tokenizer along with your prompt to the `share.prompt` function. The `max_new_tokens` keyword argument controls how many new tokens the model will generate at maximum. Experiment with different values for this argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e23d40-9f72-4cd4-b5aa-97eab4237c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# local imports\n",
    "import share\n",
    "\n",
    "model = share.load_model(share.TINYLLAMA_MODEL_DIR)\n",
    "tokenizer = share.load_tokenizer(share.TINYLLAMA_MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dfe9905-2d2c-4de6-a4d3-8e1f4d6eac28",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Hello, world!\"\n",
    "for max_new_tokens in (...): # max_new_tokens values here\n",
    "    print(f\"max. {max_new_tokens} new tokens: {share.prompt(model, tokenizer, text, max_new_tokens=max_new_tokens)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6c9024-20a4-4a21-995c-05dc6d0d86fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unload model\n",
    "del model\n",
    "gc.collect()\n",
    "torch.cuda.clear_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2305d5b2-a606-4ba5-bf8c-86980a5845cf",
   "metadata": {},
   "source": [
    "## Exercise 4: Exploring a dataset\n",
    "\n",
    "The `share` module provides pre-defined variables and helper functions for the exercises. In this exercise, the enron_spam dataset has been loaded using the `load_test_dataset` function.\n",
    "\n",
    "`test_dataset` is an instance of the [Dataset](https://huggingface.co/docs/datasets/package_reference/main_classes#datasets.Dataset) class from the [datasets](https://huggingface.co/docs/datasets/index) library.\n",
    "\n",
    "The `num_columns`, `num_rows` and `column_names` properties let you explore the dataset. It uses the `sort` method to sort by the `Spam/Ham` column and print the `text` column of the first 3 spam e-mails. It uses Python list slicing to select values at multiple rows at once (e.g. `test_dataset[:2][\"Spam/Ham\"]`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d382d9-4d16-4c81-b2ba-4dc2a34f5ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard library imports\n",
    "import os\n",
    "\n",
    "# local imports\n",
    "import share\n",
    "\n",
    "test_dataset = share.load_test_dataset(share.ENRON_SPAM_TEST_DATASET)\n",
    "\n",
    "print(f\"Number of rows: {test_dataset.num_rows}\")\n",
    "print(f\"Number of columns: {test_dataset.num_columns}\")\n",
    "print(\"Column names: \" + \", \".join(f\"'{column_name}'\" for column_name in test_dataset.column_names))\n",
    "\n",
    "for i, message in enumerate(test_dataset.sort(\"Spam/Ham\", reverse=True)[:3], start=1):\n",
    "    print(f\"{os.linesep}Message #{i}: {message}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
