{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf9ff84a-8807-4ed0-9552-25467818462a",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02b062c-9224-4ad7-ac6d-0a72970de8b8",
   "metadata": {},
   "source": [
    "## Exercise 1: Base model\n",
    "\n",
    "Use the `share.prompt` function to test the base model's capacity for classifying spam, either by copying a text from the `SetFit/enron_spam` dataset or by writing your own. The source code below wraps your text in an appropriate instruction. Feel free to change both the text and the instruction to experiment with the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f9d6a6-b23e-46c9-8298-5ffac96bff4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc, os, share, evaluation, torch\n",
    "\n",
    "# load model\n",
    "model = share.load_model(share.LLAMA2_MODEL_DIR)\n",
    "tokenizer = share.load_tokenizer(share.LLAMA2_MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691ec6f2-6e10-4d99-92d0-c24bd1069ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ... # your text here\n",
    "text_template = f\"Input:{text}{2 * os.linesep}Instruction: Output '1' if the following e-mail is spam and '0' if not. Answer in 1 token only.{2 * os.linesep}Output:\"\n",
    "print(share.prompt(model, tokenizer, text_template))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd97ad1-3d3b-41e1-ab60-bbeac9975c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unload model\n",
    "del model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e8bc62-be5b-4112-ab72-d17fb759471b",
   "metadata": {},
   "source": [
    "## Exercise 2: Fine-tuned model\n",
    "\n",
    "The Llama 2 model at `share.LLAMA2_ENRON_SPAM_LORA_MODEL_DIR` model has been fine-tuned on the `SetFit/enron_spam` dataset.\n",
    "\n",
    "As before, use the `share.prompt` function to test the fine-tuned model's capablities by hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e3a2e2-8db0-463a-b506-7e5e234aa07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc, os, share, evaluation, torch\n",
    "\n",
    "# load model\n",
    "model = share.load_model(share.LLAMA2_ENRON_SPAM_LORA_MODEL_DIR)\n",
    "tokenizer = share.load_tokenizer(share.LLAMA2_ENRON_SPAM_LORA_MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce97b54-e821-42f3-af19-58fe49ba1350",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ... # your text here\n",
    "text_template = f\"Input:{text}{2 * os.linesep}Instruction: Output '1' if the following e-mail is spam and '0' if not. Answer in 1 token only.{2 * os.linesep}Output:\"\n",
    "print(share.prompt(model, tokenizer, text_template))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5284335-0d47-43c1-bcb9-d201cad55021",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unload model\n",
    "del model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ec30e9-bb61-4e0d-a1a2-d947b80887f2",
   "metadata": {},
   "source": [
    "## Exercise 3: Automated evaluation of the base model\n",
    "\n",
    "The `evaluation.eval_precision_recall_f1` function from the `evaluation` module computes precision, recall and the F1 score for the given model on the `SetFit/enron_spam`. The keyword argument `test_size` allows you to determine the size of the test dataset.\n",
    "\n",
    "Run the function for the Llama2 base model (`share.LLAMA2_MODEL_DIR`) and experiment with different values for the `test_size` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c59cf54-be75-4d3e-a3d4-848e4b54b483",
   "metadata": {},
   "outputs": [],
   "source": [
    "import share, evaluation\n",
    "\n",
    "# load the model\n",
    "model = share.load_model(share.LLAMA2_MODEL_DIR)\n",
    "tokenizer = share.load_tokenizer(share.LLAMA2_MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fdb0be9-9363-4c45-9b70-99f8e18770c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_base_model = {}\n",
    "for test_size in (...): # add test sizes here\n",
    "    metrics_base_model[test_size] = evaluation.eval_precision_recall_f1(model, tokenizer, test_size=test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5d6498-98ab-4c6b-a433-84f49a316aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy, matplotlib.pyplot\n",
    "\n",
    "test_sizes = sorted(metrics_base_model)\n",
    "\n",
    "precision = [metrics_base_model[k][\"precision\"] for k in test_sizes]\n",
    "recall = [metrics_base_model[k][\"recall\"] for k in test_sizes]\n",
    "f1 = [metrics_base_model[k][\"f1\"] for k in test_sizes]\n",
    "\n",
    "metrics = {\n",
    "    \"Precision\": precision,\n",
    "    \"Recall\": recall,\n",
    "    \"F1 score\": f1,\n",
    "}\n",
    "x = numpy.arange(len(test_sizes))\n",
    "width = 0.25\n",
    "multiplier = 0\n",
    "fig, ax = matplotlib.pyplot.subplots(layout=\"constrained\")\n",
    "for metric, measurement in metrics.items():\n",
    "    offset = width * multiplier\n",
    "    rects = ax.bar(x + offset, measurement, width, label=metric)\n",
    "    ax.bar_label(rects, padding=3)\n",
    "    multiplier += 1\n",
    "ax.set_xticks(x + width, test_sizes)\n",
    "ax.legend(loc=\"upper left\", ncols=3)\n",
    "ax.set_ylim(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3cec62f-ad13-445e-9c97-942131e230ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc, torch\n",
    "\n",
    "# unload the model\n",
    "del model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d09034e-10bd-4eb7-b448-7443838c2205",
   "metadata": {},
   "source": [
    "## Exercise 4: Automated evaluation of the fine-tuned model\n",
    "\n",
    "Run the `evaluation.eval_precision_recall_f1` function again for the fine-tuned model model and the same `test_size` values, and compare the results for the fine-tuned model. Compare then the results for one of the values for `test_size`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c153ffc-f88e-443e-b0e5-c9ed1875b5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import share, evaluation\n",
    "\n",
    "# load the model\n",
    "model = share.load_model(share.LLAMA2_ENRON_SPAM_LORA_MODEL_DIR)\n",
    "tokenizer = share.load_tokenizer(share.LLAMA2_ENRON_SPAM_LORA_MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df40f02e-204c-44f0-9f70-ba495105873a",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_fine_tuned_model = {}\n",
    "for test_size in (...): # add test sizes here\n",
    "    metrics_fine_tuned_model[test_size] = evaluation.eval_precision_recall_f1(model, tokenizer, test_size=test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13dd58c8-9db2-4ead-87bd-7bc705341707",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy, matplotlib.pyplot\n",
    "\n",
    "test_sizes = sorted(metrics_fine_tuned_model)\n",
    "\n",
    "precision = [metrics_fine_tuned_model[k][\"precision\"] for k in test_sizes]\n",
    "recall = [metrics_fine_tuned_model[k][\"recall\"] for k in test_sizes]\n",
    "f1 = [metrics_fine_tuned_model[k][\"f1\"] for k in test_sizes]\n",
    "\n",
    "metrics = {\n",
    "    \"Precision\": precision,\n",
    "    \"Recall\": recall,\n",
    "    \"F1 score\": f1,\n",
    "}\n",
    "x = numpy.arange(len(test_sizes))\n",
    "width = 0.25\n",
    "multiplier = 0\n",
    "fig, ax = matplotlib.pyplot.subplots(layout=\"constrained\")\n",
    "for metric, measurement in metrics.items():\n",
    "    offset = width * multiplier\n",
    "    rects = ax.bar(x + offset, measurement, width, label=metric)\n",
    "    ax.bar_label(rects, padding=3)\n",
    "    multiplier += 1\n",
    "ax.set_xticks(x + width, test_sizes)\n",
    "ax.legend(loc=\"upper left\", ncols=3)\n",
    "ax.set_ylim(0, 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9e04b8-6e1a-4867-aa4f-d7e11706ffd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy, matplotlib.pyplot\n",
    "\n",
    "test_size = 32 # test size to compare here\n",
    "\n",
    "metrics = {\n",
    "    \"Base model\": [metrics_base_model[test_size][k] for k in (\"precision\", \"recall\", \"f1\")],\n",
    "    \"Fine-tuned model\": [metrics_fine_tuned_model[test_size][k] for k in (\"precision\", \"recall\", \"f1\")]\n",
    "}\n",
    "\n",
    "x = numpy.arange(len((\"precision\", \"recall\", \"f1\")))\n",
    "width = 0.25\n",
    "multiplier = 0\n",
    "fig, ax = matplotlib.pyplot.subplots(layout=\"constrained\")\n",
    "for metric, measurement in metrics.items():\n",
    "    offset = width * multiplier\n",
    "    rects = ax.bar(x + offset, measurement, width, label=metric)\n",
    "    ax.bar_label(rects, padding=3)\n",
    "    multiplier += 1\n",
    "ax.set_xticks(x + width, (\"precision\", \"recall\", \"f1\"))\n",
    "ax.legend(loc=\"upper left\", ncols=3)\n",
    "ax.set_ylim(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad8ee2d-33d9-4772-971c-b4999800f5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc, torch\n",
    "\n",
    "# unload the model\n",
    "del model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7100b90f-8917-45ee-9539-b0a5c564818a",
   "metadata": {},
   "source": [
    "## Exercise 5: Perplexity of the base model\n",
    "\n",
    "The `evaluation.eval_perplexity` function measures a model's perplexity for the `iamtarun/python_code_instructions_18k_alpaca` dataset. Run the function for the Llama 2 base model with different values for the `test_size` argument and compare the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e42543b-3351-4a89-be47-d44458386d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import share, evaluation\n",
    "\n",
    "# load the model\n",
    "model = share.load_model(share.LLAMA2_MODEL_DIR)\n",
    "tokenizer = share.load_tokenizer(share.LLAMA2_MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3570ce6-fdf6-443c-ab55-e533a7792729",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = ... # enter the size here\n",
    "metrics_base_model = evaluation.eval_perplexity(model, tokenizer, test_size=test_size)\n",
    "print(f\"Test size {test_size}:\")\n",
    "print(f\"Perplexity: {metrics_base_model['perplexity']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa05c04-95fd-4dc0-bfe6-a5e5f009d9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc, torch\n",
    "\n",
    "# unload the model\n",
    "del model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081d318e-6582-428a-a56b-439409c68d68",
   "metadata": {},
   "source": [
    "## Exercise 6: Perplexity of different fine-tuned models\n",
    "\n",
    "The\n",
    "\n",
    "* `share.LLAMA2_PYTHON_CODE_LORA_MODEL_DIR`\n",
    "* `share.LLAMA2_PYTHON_CODE_ADAPTER_MODEL_DIR`\n",
    "* `share.LLAMA2_PYTHON_CODE_FULL_MODEL_DIR`\n",
    "\n",
    "have been fine-tuned on the `iamtarun/python_code_instructions_18k_alpaca` dataset using LoRA, Llama-Adapter and full parameter fine-tuning respectively. Run `evaluation.eval_perplexity` for each one with the same value for the `test_size` argument and compare the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0227d9d4-6a81-4116-8afd-93b43e9ebfdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import share, evaluation\n",
    "\n",
    "print(f\"LoRA finetuning: {evaluation.eval_perplexity_load(share.LLAMA2_PYTHON_CODE_LORA_MODEL_DIR, test_size=32)['perplexity']}\")\n",
    "print(f\"Llama-Adapter finetuning: {evaluation.eval_perplexity_load(share.LLAMA2_PYTHON_CODE_ADAPTER_MODEL_DIR, test_size=32)['perplexity']}\")\n",
    "print(f\"Full parameter finetuning: {evaluation.eval_perplexity_load(share.LLAMA2_PYTHON_CODE_FULL_MODEL_DIR, test_size=32)['perplexity']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
