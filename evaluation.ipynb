{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf9ff84a-8807-4ed0-9552-25467818462a",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02b062c-9224-4ad7-ac6d-0a72970de8b8",
   "metadata": {},
   "source": [
    "## Exercise 1: Base model\n",
    "\n",
    "Use the `share.prompt` function to test the base model's capacity for classifying spam, either by copying a text from the `SetFit/enron_spam` dataset or by writing your own. The source code below wraps your text in an appropriate instruction. Feel free to change both the text and the instruction to experiment with the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5650f895-0b29-4234-a9f9-16f83480f048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard library imports\n",
    "import gc\n",
    "import os\n",
    "\n",
    "# local imports\n",
    "import share\n",
    "import evaluation\n",
    "\n",
    "# third party imports\n",
    "import torch\n",
    "\n",
    "\n",
    "text = ... # your text here\n",
    "text_template = f\"Input:{text}{2 * os.linesep}Instruction: Output '1' if the following e-mail is spam and '0' if not. Answer in 1 token only.{2 * os.linesep}Output:\"\n",
    "model = share.load_model(share.LLAMA2_MODEL_DIR)\n",
    "tokenizer = share.load_tokenizer(share.LLAMA2_MODEL_DIR)\n",
    "print(share.prompt(model, tokenizer, text_template))\n",
    "\n",
    "# unload model\n",
    "del model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e8bc62-be5b-4112-ab72-d17fb759471b",
   "metadata": {},
   "source": [
    "## Exercise 2: Fine-tuned model\n",
    "\n",
    "The Llama 2 model at `share.LLAMA2_ENRON_SPAM_LORA_MODEL_DIR` model has been fine-tuned on the `SetFit/enron_spam` dataset.\n",
    "\n",
    "As before, use the `share.prompt` function to test the fine-tuned model's capablities by hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f43e474-f6d2-49fc-89da-2dbbfc2e54ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard library imports\n",
    "import gc\n",
    "import os\n",
    "\n",
    "# local imports\n",
    "import share\n",
    "import evaluation\n",
    "\n",
    "# third party imports\n",
    "import torch\n",
    "\n",
    "\n",
    "text = ... # your text here\n",
    "text_template = f\"Input:{text}{2 * os.linesep}Instruction: Output '1' if the following e-mail is spam and '0' if not. Answer in 1 token only.{2 * os.linesep}Output:\"\n",
    "model = share.load_model(share.LLAMA2_ENRON_SPAM_LORA_MODEL_DIR)\n",
    "tokenizer = share.load_tokenizer(share.LLAMA2_ENRON_SPAM_LORA_MODEL_DIR)\n",
    "print(share.prompt(model, tokenizer, text_template))\n",
    "\n",
    "# unload model\n",
    "del model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ec30e9-bb61-4e0d-a1a2-d947b80887f2",
   "metadata": {},
   "source": [
    "## Exercise 3: Automatized evaluation of the base model\n",
    "\n",
    "The `evaluation.eval_precision_recall_f1` function from the `evaluation` module computes precision, recall and the F1 score for the given model on the `SetFit/enron_spam`. The keyword argument `test_size` allows you to determine the size of the test dataset.\n",
    "\n",
    "Run the function for the Llama2 base model (`share.LLAMA2_MODEL_DIR`) and experiment with different values for the `test_size` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811c7c11-6c33-407e-9ba0-6a5c5e7da7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# local imports\n",
    "import share\n",
    "import evaluation\n",
    "\n",
    "metrics_base_model = {}\n",
    "\n",
    "for test_size in (8, 16, 32): # add test sizes here\n",
    "    metrics_base_model[test_size] = evaluation.eval_precision_recall_f1(share.LLAMA2_MODEL_DIR, test_size=test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5d6498-98ab-4c6b-a433-84f49a316aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# third party imports\n",
    "import numpy\n",
    "import matplotlib.pyplot\n",
    "\n",
    "test_sizes = sorted(metrics_base_model)\n",
    "\n",
    "precision = [metrics_base_model[k][\"precision\"] for k in test_sizes]\n",
    "recall = [metrics_base_model[k][\"recall\"] for k in test_sizes]\n",
    "f1 = [metrics_base_model[k][\"f1\"] for k in test_sizes]\n",
    "\n",
    "metrics = {\n",
    "    \"Precision\": precision,\n",
    "    \"Recall\": recall,\n",
    "    \"F1 score\": f1,\n",
    "}\n",
    "x = numpy.arange(len(test_sizes))\n",
    "width = 0.25\n",
    "multiplier = 0\n",
    "fig, ax = matplotlib.pyplot.subplots(layout=\"constrained\")\n",
    "for metric, measurement in metrics.items():\n",
    "    offset = width * multiplier\n",
    "    rects = ax.bar(x + offset, measurement, width, label=metric)\n",
    "    ax.bar_label(rects, padding=3)\n",
    "    multiplier += 1\n",
    "ax.set_xticks(x + width, test_sizes)\n",
    "ax.legend(loc=\"upper left\", ncols=3)\n",
    "ax.set_ylim(0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d09034e-10bd-4eb7-b448-7443838c2205",
   "metadata": {},
   "source": [
    "## Exercise 4: Automatized evaluation of the fine-tuned model\n",
    "\n",
    "Run the `evaluation.eval_precision_recall_f1` function again for the fine-tuned model model and the same `test_size` values, and compare the results for the fine-tuned model. Compare then the results for one of the values for `test_size`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238c1fda-4f81-407c-95dc-79e67a3ec0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# local imports\n",
    "import share\n",
    "import evaluation\n",
    "\n",
    "metrics_fine_tuned_model = {}\n",
    "\n",
    "for test_size in (8, 16, 32): # add test sizes here\n",
    "    metrics_fine_tuned_model[test_size] = evaluation.eval_precision_recall_f1(share.LLAMA2_ENRON_SPAM_LORA_MODEL_DIR, test_size=test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13dd58c8-9db2-4ead-87bd-7bc705341707",
   "metadata": {},
   "outputs": [],
   "source": [
    "# third party imports\n",
    "import numpy\n",
    "import matplotlib.pyplot\n",
    "\n",
    "test_sizes = sorted(metrics_fine_tuned_model)\n",
    "\n",
    "precision = [metrics_fine_tuned_model[k][\"precision\"] for k in test_sizes]\n",
    "recall = [metrics_fine_tuned_model[k][\"recall\"] for k in test_sizes]\n",
    "f1 = [metrics_fine_tuned_model[k][\"f1\"] for k in test_sizes]\n",
    "\n",
    "metrics = {\n",
    "    \"Precision\": precision,\n",
    "    \"Recall\": recall,\n",
    "    \"F1 score\": f1,\n",
    "}\n",
    "x = numpy.arange(len(test_sizes))\n",
    "width = 0.25\n",
    "multiplier = 0\n",
    "fig, ax = matplotlib.pyplot.subplots(layout=\"constrained\")\n",
    "for metric, measurement in metrics.items():\n",
    "    offset = width * multiplier\n",
    "    rects = ax.bar(x + offset, measurement, width, label=metric)\n",
    "    ax.bar_label(rects, padding=3)\n",
    "    multiplier += 1\n",
    "ax.set_xticks(x + width, test_sizes)\n",
    "ax.legend(loc=\"upper left\", ncols=3)\n",
    "ax.set_ylim(0, 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9e04b8-6e1a-4867-aa4f-d7e11706ffd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# third party imports\n",
    "import numpy\n",
    "import matplotlib.pyplot\n",
    "\n",
    "test_size = 32 # test size to compare here\n",
    "\n",
    "metrics = {\n",
    "    \"Base model\": [metrics_base_model[test_size][k] for k in (\"precision\", \"recall\", \"f1\")],\n",
    "    \"Fine-tuned model\": [metrics_fine_tuned_model[test_size][k] for k in (\"precision\", \"recall\", \"f1\")]\n",
    "}\n",
    "\n",
    "x = numpy.arange(len((\"precision\", \"recall\", \"f1\")))\n",
    "width = 0.25\n",
    "multiplier = 0\n",
    "fig, ax = matplotlib.pyplot.subplots(layout=\"constrained\")\n",
    "for metric, measurement in metrics.items():\n",
    "    offset = width * multiplier\n",
    "    rects = ax.bar(x + offset, measurement, width, label=metric)\n",
    "    ax.bar_label(rects, padding=3)\n",
    "    multiplier += 1\n",
    "ax.set_xticks(x + width, (\"precision\", \"recall\", \"f1\"))\n",
    "ax.legend(loc=\"upper left\", ncols=3)\n",
    "ax.set_ylim(0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7100b90f-8917-45ee-9539-b0a5c564818a",
   "metadata": {},
   "source": [
    "## Exercise 5: Perplexity of the base model\n",
    "\n",
    "The `evaluation.eval_perplexity` function measures a model's perplexity for the `iamtarun/python_code_instructions_18k_alpaca` dataset. Run the function for the Llama 2 base model with different values for the `test_size` argument and compare the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3f6b5c-1f69-4208-a719-d5948249a0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# local imports\n",
    "import share\n",
    "import evaluation\n",
    "\n",
    "metrics_base_model = evaluation.eval_perplexity(share.LLAMA2_MODEL_DIR, test_size=...)\n",
    "print(f\"Test size {test_size}:\")\n",
    "print(f\"Perplexity: {metrics_base_model['perplexity']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081d318e-6582-428a-a56b-439409c68d68",
   "metadata": {},
   "source": [
    "## Exercise 5: Perplexity of different fine-tuned models\n",
    "\n",
    "The\n",
    "\n",
    "* `share.LLAMA2_PYTHON_CODE_LORA_MODEL_DIR`\n",
    "* `share.LLAMA2_PYTHON_CODE_ADAPTER_MODEL_DIR`\n",
    "* `share.LLAMA2_PYTHON_CODE_FULL_MODEL_DIR`\n",
    "\n",
    "have been fine-tuned on the `iamtarun/python_code_instructions_18k_alpaca` dataset using LoRA, Llama-Adapter and full parameter fine-tuning respectively. Run `evaluation.eval_perplexity` for each one with the same value for the `test_size` argument and compare the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0227d9d4-6a81-4116-8afd-93b43e9ebfdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# local imports\n",
    "import share\n",
    "import evaluation\n",
    "\n",
    "print(f\"LoRA finetuning: {evaluation.eval_perplexity(share.LLAMA2_PYTHON_CODE_LORA_MODEL_DIR, test_size=32)['perplexity']}\")\n",
    "print(f\"Llama-Adapter finetuning: {evaluation.eval_perplexity(share.LLAMA2_PYTHON_CODE_ADAPTER_MODEL_DIR, test_size=32)['perplexity']}\")\n",
    "print(f\"Full parameter finetuning: {evaluation.eval_perplexity(share.LLAMA2_PYTHON_CODE_FULL_MODEL_DIR, test_size=32)['perplexity']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
